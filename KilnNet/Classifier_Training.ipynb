{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:17:45.753843Z",
     "start_time": "2018-02-11T20:16:53.081608Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet import ResNet50\n",
    "from keras.applications.resnet import ResNet101\n",
    "from keras.applications.resnet import ResNet152\n",
    "from keras.applications.resnet_v2 import ResNet50V2\n",
    "from keras.applications.resnet_v2 import ResNet101V2\n",
    "from keras.applications.resnet_v2 import ResNet152V2\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.densenet import DenseNet169\n",
    "from keras.applications.densenet import DenseNet201\n",
    "from keras.activations import softmax, relu, sigmoid\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard,ReduceLROnPlateau,CSVLogger, EarlyStopping\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2\n",
    "from random import shuffle\n",
    "import random\n",
    "import glob\n",
    "from skimage.transform import resize\n",
    "#from PIL import Image\n",
    "import os\n",
    "#from model import *\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T09:56:34.324631Z",
     "start_time": "2018-02-12T09:56:34.321673Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'VGG16_Jan2020'\n",
    "num_classes = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data_X, data_y, batch_size):\n",
    "    indexes = np.array(range(len(data_y)))\n",
    "    n = len(indexes)\n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        np.random.shuffle(indexes)\n",
    "        while batch_start < n:\n",
    "            index = []\n",
    "            batch_y = []\n",
    "            y = []\n",
    "            index = indexes[batch_start:batch_end]\n",
    "            batch_x = np.array([data_X[i] for i in index])\n",
    "            batch_y = np.array([data_y[i] for i in index])\n",
    "            yield batch_x, batch_y\n",
    "            batch_start += batch_size   \n",
    "            batch_end += batch_size\n",
    "            if (batch_end>len(data_y)):\n",
    "                batch_end = len(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_landUse(dataset_path, im_type):\n",
    "    import glob\n",
    "    paths_X = []   \n",
    "    labels = [] \n",
    "    i = 0\n",
    "    folders = sorted(os.listdir(dataset_path))\n",
    "    for folder in folders:\n",
    "        temp = sorted (glob.glob(os.path.join(dataset_path,folder+'/*'+im_type)))\n",
    "        for k in range(len(temp)):\n",
    "            labels.append (i)\n",
    "        paths_X += temp\n",
    "        i += 1\n",
    "        \n",
    "    if(len(paths_X)==0):\n",
    "        print ('Dataset could not found. Please provide correct path.')\n",
    "    return paths_X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'Dataset/train/'\n",
    "paths, labels = read_landUse(dataset_path, '[(.png)(.jpg)]')\n",
    "images = np.array([cv2.imread(image_name) for image_name in paths]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T10:30:03.675244Z",
     "start_time": "2018-02-12T10:30:00.020199Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = images\n",
    "y_train = to_categorical(labels, num_classes)\n",
    "#X, y = load_dataset(dataset_path)\n",
    "print ('Size of dataset:', len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_val = 'Dataset/val/'\n",
    "paths_val, labels_val = read_landUse(dataset_path_val, '[(.png)(.jpg)]')\n",
    "X_val =  np.array([cv2.imread(image_name) for image_name in paths_val]) \n",
    "y_val = to_categorical(labels_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folders = os.listdir(dataset_path)\n",
    "names, counts = [], []\n",
    "for i, folder in enumerate(folders):\n",
    "    dir_path = os.path.join(dataset_path, folder)\n",
    "    images = os.listdir(dir_path)\n",
    "    names.append(folder)\n",
    "    counts.append(len(images))\n",
    "    print(i, folder, len(images))\n",
    "df = pd.DataFrame(data={'Name': names, 'Count': counts})\n",
    "ax = df.plot(kind='bar', xticks=counts, grid=True, legend=False, figsize=(16,6))\n",
    "ax.set_xlabel('Classes', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "plt.xticks(np.arange(len(names)), names, rotation=45, fontsize=15)\n",
    "for i, v in enumerate(counts):\n",
    "    ax.text(i-.18, v+15, str(v), color='blue', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T10:30:09.910515Z",
     "start_time": "2018-02-12T10:30:09.897701Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_gen = batch_generator(X_train, y_train, batch_size)\n",
    "val_gen = batch_generator(X_val, y_val, batch_size)\n",
    "#test_gen = batch_generator(X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T10:30:19.400534Z",
     "start_time": "2018-02-12T10:30:11.306984Z"
    }
   },
   "outputs": [],
   "source": [
    "input_shape=(256,256,3)\n",
    "model= VGG16(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= VGG19(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= ResNet50(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#= ResNet101(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= ResNet152(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= ResNet50V2(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= ResNet101V2(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= ResNet152V2(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= InceptionResNetV2(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= InceptionV3(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= DenseNet121(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= DenseNet169(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model= DenseNet201(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model =InceptionResNetV2(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "#model = resNet(input_shape, num_classes, model_type='resnet_152')\n",
    "#model.load_weights('WEIGHTS/'+model_name+'.h5')\n",
    "\n",
    "### model_vgg16.layers[-1].activation = relu\n",
    "\n",
    "### idx_of_layer_to_change = -1\n",
    "#model.layers[idx_of_layer_to_change].activation = activations.softmax\n",
    "#model = utils.apply_modifications(model)\n",
    "\n",
    "#model_vgg16.layers[-1].activation = relu\n",
    "# # FINE TUNING HERE\n",
    "# top_model = Sequential()\n",
    "# top_model.add(Dense(input_shape=model.layers[-2].output_shape, units=num_classes, rnel\n",
    "# kernel_initializer=\"he_normal\", activation=\"softmax\"))\n",
    "\n",
    "# model.layers.pop()\n",
    "# model.outputs = [model.layers[-1].output]\n",
    "# model.layers[-1].outbound_nodes = []\n",
    "\n",
    "# model = Model(inputs=model.inputs, outputs=top_model(model.outputs[0]))\n",
    "# # for layer in model.layers[:-1]:\n",
    "# #     layer.trainable = False\n",
    "#model.load_weights('WEIGHTS/'+model_name+'.h5')\n",
    "\n",
    "model.summary()\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# plot_model(model, to_file='model.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 629/650, \n",
    "                 1: 629/635, \n",
    "                 2: 629/634, \n",
    "                 3: 629/630, \n",
    "                 4: 629/629, \n",
    "                 5: 629/633, \n",
    "                 6: 629/629, \n",
    "                 7: 629/620, \n",
    "                 8: 629/632, \n",
    "                 9: 629/617, \n",
    "                 10: 629/623,\n",
    "                 11:629/638,\n",
    "                 12:629/635,\n",
    "                 13:629/524\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ResNet-152 network using \"fit_generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T13:36:14.611256Z",
     "start_time": "2018-02-12T12:07:26.159215Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('WEIGHTS/'+model_name+'.h5', monitor='val_loss', verbose=2, save_best_only=True, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir='EVENTS/', batch_size=batch_size, write_graph=True, write_images=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=3, min_lr=0.00001)\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=2, mode='auto')\n",
    "cvslogger = keras.callbacks.CSVLogger('WEIGHTS/'+model_name+'.csv', separator=',', append=True)\n",
    " \n",
    "nb_epoch = 200\n",
    "callbacks = [checkpoint, reduce_lr, earlystop, cvslogger, tensorboard]\n",
    "train_steps = int(len(y_train)//batch_size)\n",
    "val_steps = int(len(y_val)//batch_size)\n",
    "start_time = time.time()\n",
    "history = model.fit_generator(train_gen, train_steps, epochs=nb_epoch, verbose=1, \n",
    "                    max_queue_size=2, validation_data=val_gen, \n",
    "                    validation_steps=val_steps, shuffle=True,\n",
    "                    workers=1, use_multiprocessing= False, \n",
    "                    initial_epoch=0, callbacks=callbacks, class_weight=class_weights)\n",
    "print(\"Seconds: \", time.time() - start_time)\n",
    "#model.fit(x=X[0:100], y=y[0:100], batch_size=64, epochs=2, verbose=1, callbacks=callbacks, \n",
    "#          validation_split=0.2, shuffle=True,initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
