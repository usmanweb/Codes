{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:17:45.753843Z",
     "start_time": "2018-02-11T20:16:53.081608Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import cv2\n",
    "from random import shuffle\n",
    "import random\n",
    "import glob\n",
    "from skimage.transform import resize\n",
    "#from PIL import Image\n",
    "import os\n",
    "from model import *\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from inception_resnet_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T09:56:34.324631Z",
     "start_time": "2018-02-12T09:56:34.321673Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T09:56:38.586840Z",
     "start_time": "2018-02-12T09:56:38.459036Z"
    },
    "code_folding": [
     0,
     36,
     55
    ]
   },
   "outputs": [],
   "source": [
    "def batch_generator(data_X, data_y, batch_size):\n",
    "    indexes = np.array(range(len(data_y)))\n",
    "    n = len(indexes)\n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        np.random.shuffle(indexes)\n",
    "        while batch_start < n:\n",
    "            index = []\n",
    "            batch_y = []\n",
    "            y = []\n",
    "            index = indexes[batch_start:batch_end]\n",
    "            batch_x = np.array([data_X[i] for i in index])\n",
    "            batch_y = np.array([data_y[i] for i in index])\n",
    "            yield batch_x, batch_y\n",
    "            batch_start += batch_size   \n",
    "            batch_end += batch_size\n",
    "            if (batch_end>len(data_y)):\n",
    "                batch_end = len(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_landUse(dataset_path, im_type):\n",
    "    import glob\n",
    "    paths_X = []   \n",
    "    labels = [] \n",
    "    i = 0\n",
    "    folders = sorted(os.listdir(dataset_path))\n",
    "    for folder in folders:\n",
    "        temp = sorted (glob.glob(os.path.join(dataset_path,folder+'/*'+im_type)))\n",
    "        for k in range(len(temp)):\n",
    "            labels.append (i)\n",
    "        paths_X += temp\n",
    "        i += 1\n",
    "        \n",
    "    if(len(paths_X)==0):\n",
    "        print ('Dataset could not found. Please provide correct path.')\n",
    "    return paths_X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'train'\n",
    "\n",
    "paths, labels = read_landUse(dataset_path, '[(.png)(.jpg)]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(np.array(cv2.imread(paths[0])))\n",
    "images = np.array([cv2.imread(image_name) for image_name in paths]) \n",
    "#images[0]\n",
    "#np.shape(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'InceptionResNet-v2'\n",
    "#dataset_path = '/media/waseem/3D0A87CA7D9EC477/M.Bhimra/mohbat_resnet/FINAL DATASET/dataset/houses'\n",
    "#model_name = 'resNet152_retrain_less'\n",
    "#dataset_path = 'train-64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T10:30:03.675244Z",
     "start_time": "2018-02-12T10:30:00.020199Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = images\n",
    "y = to_categorical(labels, num_classes)\n",
    "#X, y = load_dataset(dataset_path)\n",
    "print ('Size of dataset:', len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir(dataset_path)\n",
    "names, counts = [], []\n",
    "for i, folder in enumerate(folders):\n",
    "    dir_path = os.path.join(dataset_path, folder)\n",
    "    images1 = os.listdir(dir_path)\n",
    "    names.append(folder)\n",
    "    counts.append(len(images1))\n",
    "    print(i, folder, len(images1))\n",
    "df = pd.DataFrame(data={'Name': names, 'Count': counts})\n",
    "ax = df.plot(kind='bar', xticks=counts, grid=True, legend=False, figsize=(16,6))\n",
    "ax.set_xlabel('Classes', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "plt.xticks(np.arange(len(names)), names, rotation=45, fontsize=15)\n",
    "for i, v in enumerate(counts):\n",
    "    ax.text(i-.18, v+15, str(v), color='blue', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T10:30:19.400534Z",
     "start_time": "2018-02-12T10:30:11.306984Z"
    }
   },
   "outputs": [],
   "source": [
    "input_shape=(256,256,3)\n",
    "# The '-1' is because we add a new class for finetuning\n",
    "#model = resNet(input_shape, num_classes, model_type='resnet_152')\n",
    "#model = VGG16(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "model =InceptionResNetV2(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "\n",
    "#model = resNet(input_shape, num_classes, model_type='resnet_34')\n",
    "#model.load_weights(model_name+'.h5')\n",
    "# model.load_weights('resNet152 (another copy).h5')\n",
    "\n",
    "# # FINE TUNING HERE\n",
    "# top_model = Sequential()\n",
    "# top_model.add(Dense(input_shape=model.layers[-2].output_shape, units=num_classes, rnel\n",
    "# kernel_initializer=\"he_normal\", activation=\"softmax\"))\n",
    "\n",
    "# model.layers.pop()\n",
    "# model.outputs = [model.layers[-1].output]\n",
    "# model.layers[-1].outbound_nodes = []\n",
    "\n",
    "# model = Model(inputs=model.inputs, outputs=top_model(model.outputs[0]))\n",
    "# # for layer in model.layers[:-1]:\n",
    "# #     layer.trainable = False\n",
    "\n",
    "model.summary()\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# plot_model(model, to_file='model.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 648/618,\n",
    "                1: 648/600,\n",
    "                2: 648/599,\n",
    "                3: 648/599,\n",
    "                4: 648/594,\n",
    "                5: 648/603,\n",
    "                6: 648/648,\n",
    "                7: 648/593,\n",
    "                8: 648/582,\n",
    "                9: 648/600,\n",
    "               10: 648/742\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ResNet-152 network using \"fit_generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T13:36:14.611256Z",
     "start_time": "2018-02-12T12:07:26.159215Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('WEIGHTS/'+model_name+'.h5', monitor='val_loss', verbose=2, save_best_only=True, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir='TENSORBOARD/', batch_size=batch_size, write_graph=True, write_images=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=3, min_lr=0.00001)\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=2, mode='auto')\n",
    "cvslogger = keras.callbacks.CSVLogger('WEIGHTS/'+model_name+'.csv', separator=',', append=True)\n",
    "nb_epoch = 200\n",
    "callbacks = [checkpoint, reduce_lr, earlystop, cvslogger, tensorboard]\n",
    "#train_steps = int(len(y_train)//batch_size)\n",
    "#val_steps = int(len(y_test)//batch_size)\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "#print(kf)  \n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_gen = batch_generator(X_train, y_train, batch_size)\n",
    "    test_gen = batch_generator(X_test, y_test, batch_size)\n",
    "    history = model.fit_generator(train_gen, int(len(y_train)//batch_size), epochs=nb_epoch, verbose=1, \n",
    "                    max_queue_size=2, validation_data=test_gen, \n",
    "                    validation_steps=int(len(y_test)//batch_size), shuffle=True,\n",
    "                    workers=1, use_multiprocessing= False, \n",
    "                    initial_epoch=0, callbacks=callbacks, class_weight=class_weights)\n",
    "    #model.evaluate_generator(test_gen, steps=len(X[test])//batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(test_gen, steps=len(X_test)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuned_model(model, x):\n",
    "\n",
    "    t_model = Model(model.input, x)\n",
    "    \n",
    "    return t_model\n",
    "\n",
    "\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "x = layer_dict['avg_pool'].output \n",
    "\n",
    "t_model = tuned_model(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_output = t_model.predict(X)\n",
    "val_labels= np.argmax(y, axis=1)\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "from tsne import bh_sne\n",
    "\n",
    "X = np.asarray(p_output).astype('float64')\n",
    "X = X.reshape((p_output.shape[0], -1))\n",
    "# For speed of computation, only run on a subset\n",
    "#n = 20000\n",
    "#X_train_data = X_train[:n]\n",
    "#Y_train_data = y_train[:n]\n",
    "# perform t-SNE embedding\n",
    "vis_data = bh_sne(X)\n",
    "# plot the result\n",
    "vis_x = vis_data[:, 0]\n",
    "vis_y = vis_data[:, 1]\n",
    "f = plt.figure()\n",
    "plt.scatter(vis_x, vis_y, c=val_labels, cmap=plt.cm.get_cmap(\"jet\", 11), s=10)\n",
    "plt.colorbar(ticks=range(11))\n",
    "plt.show()\n",
    "f.savefig(\"tsne_BrickKilns.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tdataset_path = 'test'\n",
    "Tpaths, Tlabels = read_landUse(Tdataset_path, '[(.png)(.jpg)]')\n",
    "Timages = np.array([cv2.imread(image_name) for image_name in Tpaths]) \n",
    "X_test = Timages\n",
    "y_test = to_categorical(Tlabels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "class_labels = {0: 'blackfarms', 1: 'densetrees', 2: 'farms', 3: 'grass', 4: 'ground',5: 'houses',6:'kiln',7:'orchard',8:'parking',9:'roads',10:'unevenland'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=y_pred\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_classes = []\n",
    "for p in preds.argmax(axis=1):\n",
    "    pres_classes.append(class_labels[p])\n",
    "#print(pres_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_classes = []\n",
    "for p in y_test.argmax(axis=1):\n",
    "    truth_classes.append(class_labels[p])\n",
    "#print(truth_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_confusion_matrix(y_test, y_pred, classes):\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=classes,title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()\n",
    "    return cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = my_confusion_matrix(truth_classes,pres_classes,class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(label, confusion_matrix):\n",
    "    col = confusion_matrix[:, label]\n",
    "    return confusion_matrix[label, label] / col.sum()\n",
    "def recall(label, confusion_matrix):\n",
    "    row = confusion_matrix[label, :]\n",
    "    return confusion_matrix[label, label] / row.sum()\n",
    "# Precision Calculation https://www.python-course.eu/confusion_matrix.php\n",
    "# Precision Calculation https://www.python-course.eu/confusion_matrix.php\n",
    "a = precision(6,cm)\n",
    "b = recall(6,cm)\n",
    "print('Precision', a)\n",
    "print('Recall', b)\n",
    "print('F1', (2*a*b)/(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_activation(activations, col_size, row_size, act_index): \n",
    "    activation = activations[act_index]\n",
    "    activation_index=0\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "            activation_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model(\"InceptionResNet-v2.h5\")\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(X[1].reshape(1,24,64,1))\n",
    "display_activation(activations, 3, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = model.layers[1]\n",
    "plt.imshow(l1.get_weights()[0][:,:,0,0].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = model.layers[4]\n",
    "plt.imshow(l2.get_weights()[0][:,:,0,0].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[:12]] # Extracts the outputs of the top 12 layers\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\n",
    "activations = activation_model.predict([test_data_1[0].reshape((1,22,22,1)),test_data_2[0].reshape((1,22,22,1))]) # Returns a list of five Numpy arrays: one array per layer activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_activation = activations[3]\n",
    "print(first_layer_activation.shape)\n",
    "plt.matshow(first_layer_activation[0, :, :, 0], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T14:05:04.763990Z",
     "start_time": "2018-02-12T14:05:04.749105Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "# test_images_dir = 'lhr/newtask'\n",
    "test_images_dir = 'Kabul2NepalDatasets/Kabul2NepalZoom20Y2018_1_1000_Images/'\n",
    "\n",
    "# results_dir = 'results_lahore_17_with_old_wets_attempt2'\n",
    "results_dir = 'Results/Kabul2NepalZoom20Y2018_1_1000_Images/'\n",
    "if not os.path.exists(results_dir):\n",
    "    print('Creating new results directory \"{}\"'.format(results_dir))\n",
    "    os.mkdir(results_dir)\n",
    "im_dir = os.path.join(results_dir, 'images')\n",
    "if not os.path.exists(im_dir):\n",
    "    os.mkdir(im_dir)\n",
    "\n",
    "#for file in os.listdir(test_images_dir):\n",
    "#    curr_name, curr_ext = os.path.splitext(file)\n",
    " #   print(curr_name[:-1])\n",
    " #   break\n",
    "#    new_name = curr_name[:-1]\n",
    "#    os.rename(os.path.join(test_images_dir, file), os.path.join(test_images_dir, new_name+curr_ext))\n",
    "\n",
    "########################################################\n",
    "########### RENAMING DONE IN DOWNLOAD SCRIPT ###########\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model and creating result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T14:05:48.240321Z",
     "start_time": "2018-02-12T14:05:07.149663Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## print('Waiting'len(images)len(images)len(images), end='', flush=True)len(images)\n",
    "# while True:\n",
    "    \n",
    " #   if len(os.listdir(test_images_dir)) != 544244:\n",
    " #      time.sleep(10*60)\n",
    " #       print('.', end='', flush=True)\n",
    " #       continue\n",
    "\n",
    "\n",
    "# if os.path.exists(os.path.join(results_dir, 'houses.txt')) or os.path.exists(os.path.join(results_dir, 'test.csv')):\n",
    "#     raise OSError('Result files already present, this script will append to the existing data.')\n",
    "    \n",
    "# label_names = {0: 'parking', 1: 'parks', 2: 'ground', 3: 'houses', 4: 'roads', 5: 'mosque', 6: 'densetrees', 7: 'kiln', 8: 'oiltanks', 9: 'tennis', 10: 'ponds', 11: 'grass', 12: 'blackfarms', 13: 'farms', 14: 'orchard'}\n",
    "# label_names = {0: 'parking', 1: 'parks', 2: 'ground', 3: 'houses', 4: 'roads', 5: 'mosque', 6: 'densetrees', 7: 'kiln', 8: 'oiltanks', 9: 'tennis', 10: 'ponds', 11: 'grass', 12: 'blackfarms', 13: 'farms'}\n",
    "# label_names = {0: 'blackfarms', 1: 'densetrees', 2: 'farms', 3: 'grass', 4: 'ground', 5: 'houses', 6: 'kiln', 7: 'mosque', 8: 'oiltanks', 9: 'orchard', 10: 'parking', 11: 'parks', 12: 'ponds', 13: 'roads', 14: 'tennis'}\n",
    "label_names = {0: 'blackfarms', 1: 'densetrees', 2: 'farms', 3: 'grass', 4: 'ground', 5: 'houses', 6: 'kiln', 7: 'orchard', 8: 'parking', 9: 'roads', 10:'unevenland'}\n",
    "y_tiles, x_tiles = [], []\n",
    "label_probs = defaultdict(list)\n",
    "top_label_probs = defaultdict(list)\n",
    "\n",
    "# # Resuming check\n",
    "# try:\n",
    "#     with open(os.path.join(results_dir, 'last_file_done.txt'), 'r') as f:\n",
    "#         loop_index = int(f.read())\n",
    "#     print('Resuming from file index \"{}\"'.format(loop_index))\n",
    "# except FileNotFoundError:\n",
    "#     loop_index = 0\n",
    "#     print('Starting fresh from file index \"0\"')\n",
    "\n",
    "filenames = os.listdir(test_images_dir)\n",
    "\n",
    "# # Missing images check\n",
    "# try:\n",
    "#     with open(os.path.join(results_dir, 'missing_images.txt'), 'r') as f:\n",
    "#         missing_images = [name.strip() for name in f.readlines()]\n",
    "#     print('Continuing with {} missing images.'.format(len(missing_images)))\n",
    "# except FileNotFoundError:\n",
    "#     missing_images = []\n",
    "#     print('Starting fresh with empty missing images list')\n",
    "for img_name in tqdm(filenames):\n",
    "\n",
    "\n",
    "#     extt=img_name.split('.')[1]\n",
    "    orig_name, extt = os.path.splitext(img_name)\n",
    "    if(extt == '.jpg'):\n",
    "        img_orig = cv2.imread(os.path.join(test_images_dir, img_name), cv2.IMREAD_COLOR)\n",
    "        # To catch corrupt images\n",
    "        if type(img_orig) == type(None):\n",
    "            print('Skipping an image \"{}\"'.format(img_name))\n",
    "        #    missing_images.append(img_name)\n",
    "            continue\n",
    "\n",
    "        img = np.expand_dims(img_orig, axis=0)\n",
    "        class_name = img_name.split('.')[0]\n",
    "\n",
    "        pred = model.predict(img, verbose=0)\n",
    "    #    preds.append((pred.argmax(), pred.max(), class_name))\n",
    "\n",
    "#         src = os.path.join(test_images_dir, img_name)\n",
    "#         dst = os.path.join(results_dir, 'images', orig_name+'_'+label_names[pred.argmax()]+extt)\n",
    "#         shutil.copyfile(src, dst)\n",
    "#         print(img_name, dst)\n",
    "\n",
    "#         break\n",
    "        x_tile, y_tile = os.path.splitext(img_name)[0].split('_')\n",
    "        y_tiles.append(y_tile)\n",
    "        x_tiles.append(x_tile)\n",
    "\n",
    "        # Append to individual results files on each itteration\n",
    "        for i, label in label_names.items():\n",
    "            file_path = os.path.join(results_dir, label+'.txt')\n",
    "            label_probs[label].append(pred[0, i])\n",
    "            with open(file_path, 'a') as f:\n",
    "                f.write('{} {} {}\\n'.format(y_tile, x_tile, pred[0, i]))\n",
    "#         # Append to full result file on every 1000 itterations\n",
    "#         #if loop_index % 1000 == 0:\n",
    "#         d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "#         d.update(label_probs)\n",
    "#         final_df = pd.DataFrame(data=d)\n",
    "#         final_df.to_csv(os.path.join(results_dir, 'test.csv'), index=False, header=True)\n",
    "        # Resuming \n",
    "        #with open('last_file_done.txt', 'w') as f:\n",
    "         #   f.write(str(loop_index))\n",
    "        # Write names of corrupted files for later use\n",
    "        #with open(os.path.join(results_dir, 'missing.txt'), 'w') as f:\n",
    "           # for i in missing_images:\n",
    "           #     f.write('{}\\n'.format(i))\n",
    "        #loop_index += 1\n",
    "\n",
    "        top3_inds = np.argsort(pred)\n",
    "        top3_inds = np.flip(top3_inds, axis=1)\n",
    "        top3_labels = [ label_names[i] for ia in top3_inds for i in ia ]\n",
    "        top3_probs = pred[0, top3_inds]\n",
    "#     print(top3_labels)\n",
    "\n",
    "        for n in range(num_classes):\n",
    "            top_label_probs['label'+str(n+1)].append(top3_labels[n])\n",
    "            top_label_probs['prob'+str(n+1)].append(top3_probs[0, n])\n",
    "\n",
    "# Append to full result file last time\n",
    "d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "d.update(label_probs)\n",
    "final_df = pd.DataFrame(data=d)\n",
    "final_df.to_csv(os.path.join(results_dir, 'test.csv'), index=False, header=True)\n",
    "# final_df\n",
    "\n",
    "# Write names of corrupted files for later use\n",
    "#with open(os.path.join(results_dir, 'missing.txt'), 'w') as f:\n",
    "#    for i in missing_images:\n",
    "#        f.write('{}\\n'.format(i))\n",
    "\n",
    "d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "d.update(top_label_probs)\n",
    "sorted_df = pd.DataFrame(data=d)\n",
    "sorted_df = sorted_df[['y_tile', 'x_tile', 'label1', 'prob1', 'label2', 'prob2', 'label3', 'prob3', 'label4', 'prob4', 'label5', 'prob5', 'label6', 'prob6', 'label7', 'prob7', 'label8', 'prob8', 'label9', 'prob9', 'label10', 'prob10']]\n",
    "sorted_df.to_csv(os.path.join(results_dir, 'top_predictions.csv'), index=False, header=True)\n",
    "# sorted_df.head()\n",
    "\n",
    "# preds = sorted(preds, key=lambda x: x[0])\n",
    "# for p_class, p_prob, truth in preds:\n",
    "#     got_it = label_names[p_class] == truth\n",
    "#     print('{}\\tPrediction: {}\\tTruth: {}\\tProb: {}'.format(got_it, label_names[p_class], truth, p_prob))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# test_images_dir = 'lhr/newtask'\n",
    "test_images_dir = 'Kabul2NepalDatasets/Kabul2NepalZoom20Y2018_1001_5000_Images/'\n",
    "\n",
    "# results_dir = 'results_lahore_17_with_old_wets_attempt2'\n",
    "results_dir = 'Results/Kabul2NepalZoom20Y2018_1001_5000_Images/'\n",
    "if not os.path.exists(results_dir):\n",
    "    print('Creating new results directory \"{}\"'.format(results_dir))\n",
    "    os.mkdir(results_dir)\n",
    "im_dir = os.path.join(results_dir, 'images')\n",
    "if not os.path.exists(im_dir):\n",
    "    os.mkdir(im_dir)\n",
    "\n",
    "#for file in os.listdir(test_images_dir):\n",
    "#    curr_name, curr_ext = os.path.splitext(file)\n",
    " #   print(curr_name[:-1])\n",
    " #   break\n",
    "#    new_name = curr_name[:-1]\n",
    "#    os.rename(os.path.join(test_images_dir, file), os.path.join(test_images_dir, new_name+curr_ext))\n",
    "\n",
    "########################################################\n",
    "########### RENAMING DONE IN DOWNLOAD SCRIPT ###########\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print('Waiting'len(images)len(images)len(images), end='', flush=True)len(images)\n",
    "# while True:\n",
    "    \n",
    " #   if len(os.listdir(test_images_dir)) != 544244:\n",
    " #      time.sleep(10*60)\n",
    " #       print('.', end='', flush=True)\n",
    " #       continue\n",
    "\n",
    "\n",
    "# if os.path.exists(os.path.join(results_dir, 'houses.txt')) or os.path.exists(os.path.join(results_dir, 'test.csv')):\n",
    "#     raise OSError('Result files already present, this script will append to the existing data.')\n",
    "    \n",
    "# label_names = {0: 'parking', 1: 'parks', 2: 'ground', 3: 'houses', 4: 'roads', 5: 'mosque', 6: 'densetrees', 7: 'kiln', 8: 'oiltanks', 9: 'tennis', 10: 'ponds', 11: 'grass', 12: 'blackfarms', 13: 'farms', 14: 'orchard'}\n",
    "# label_names = {0: 'parking', 1: 'parks', 2: 'ground', 3: 'houses', 4: 'roads', 5: 'mosque', 6: 'densetrees', 7: 'kiln', 8: 'oiltanks', 9: 'tennis', 10: 'ponds', 11: 'grass', 12: 'blackfarms', 13: 'farms'}\n",
    "# label_names = {0: 'blackfarms', 1: 'densetrees', 2: 'farms', 3: 'grass', 4: 'ground', 5: 'houses', 6: 'kiln', 7: 'mosque', 8: 'oiltanks', 9: 'orchard', 10: 'parking', 11: 'parks', 12: 'ponds', 13: 'roads', 14: 'tennis'}\n",
    "label_names = {0: 'blackfarms', 1: 'densetrees', 2: 'farms', 3: 'grass', 4: 'ground', 5: 'houses', 6: 'kiln', 7: 'orchard', 8: 'parking', 9: 'roads', 10:'unevenland'}\n",
    "y_tiles, x_tiles = [], []\n",
    "label_probs = defaultdict(list)\n",
    "top_label_probs = defaultdict(list)\n",
    "\n",
    "# # Resuming check\n",
    "# try:\n",
    "#     with open(os.path.join(results_dir, 'last_file_done.txt'), 'r') as f:\n",
    "#         loop_index = int(f.read())\n",
    "#     print('Resuming from file index \"{}\"'.format(loop_index))\n",
    "# except FileNotFoundError:\n",
    "#     loop_index = 0\n",
    "#     print('Starting fresh from file index \"0\"')\n",
    "\n",
    "filenames = os.listdir(test_images_dir)\n",
    "\n",
    "# # Missing images check\n",
    "# try:\n",
    "#     with open(os.path.join(results_dir, 'missing_images.txt'), 'r') as f:\n",
    "#         missing_images = [name.strip() for name in f.readlines()]\n",
    "#     print('Continuing with {} missing images.'.format(len(missing_images)))\n",
    "# except FileNotFoundError:\n",
    "#     missing_images = []\n",
    "#     print('Starting fresh with empty missing images list')\n",
    "for img_name in tqdm(filenames):\n",
    "\n",
    "\n",
    "#     extt=img_name.split('.')[1]\n",
    "    orig_name, extt = os.path.splitext(img_name)\n",
    "    if(extt == '.jpg'):\n",
    "        img_orig = cv2.imread(os.path.join(test_images_dir, img_name), cv2.IMREAD_COLOR)\n",
    "        # To catch corrupt images\n",
    "        if type(img_orig) == type(None):\n",
    "            print('Skipping an image \"{}\"'.format(img_name))\n",
    "        #    missing_images.append(img_name)\n",
    "            continue\n",
    "\n",
    "        img = np.expand_dims(img_orig, axis=0)\n",
    "        class_name = img_name.split('.')[0]\n",
    "\n",
    "        pred = model.predict(img, verbose=0)\n",
    "    #    preds.append((pred.argmax(), pred.max(), class_name))\n",
    "\n",
    "#         src = os.path.join(test_images_dir, img_name)\n",
    "#         dst = os.path.join(results_dir, 'images', orig_name+'_'+label_names[pred.argmax()]+extt)\n",
    "#         shutil.copyfile(src, dst)\n",
    "#         print(img_name, dst)\n",
    "\n",
    "#         break\n",
    "        x_tile, y_tile = os.path.splitext(img_name)[0].split('_')\n",
    "        y_tiles.append(y_tile)\n",
    "        x_tiles.append(x_tile)\n",
    "\n",
    "        # Append to individual results files on each itteration\n",
    "        for i, label in label_names.items():\n",
    "            file_path = os.path.join(results_dir, label+'.txt')\n",
    "            label_probs[label].append(pred[0, i])\n",
    "            with open(file_path, 'a') as f:\n",
    "                f.write('{} {} {}\\n'.format(y_tile, x_tile, pred[0, i]))\n",
    "#         # Append to full result file on every 1000 itterations\n",
    "#         #if loop_index % 1000 == 0:\n",
    "#         d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "#         d.update(label_probs)\n",
    "#         final_df = pd.DataFrame(data=d)\n",
    "#         final_df.to_csv(os.path.join(results_dir, 'test.csv'), index=False, header=True)\n",
    "        # Resuming \n",
    "        #with open('last_file_done.txt', 'w') as f:\n",
    "         #   f.write(str(loop_index))\n",
    "        # Write names of corrupted files for later use\n",
    "        #with open(os.path.join(results_dir, 'missing.txt'), 'w') as f:\n",
    "           # for i in missing_images:\n",
    "           #     f.write('{}\\n'.format(i))\n",
    "        #loop_index += 1\n",
    "\n",
    "        top3_inds = np.argsort(pred)\n",
    "        top3_inds = np.flip(top3_inds, axis=1)\n",
    "        top3_labels = [ label_names[i] for ia in top3_inds for i in ia ]\n",
    "        top3_probs = pred[0, top3_inds]\n",
    "#     print(top3_labels)\n",
    "\n",
    "        for n in range(num_classes):\n",
    "            top_label_probs['label'+str(n+1)].append(top3_labels[n])\n",
    "            top_label_probs['prob'+str(n+1)].append(top3_probs[0, n])\n",
    "\n",
    "# Append to full result file last time\n",
    "d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "d.update(label_probs)\n",
    "final_df = pd.DataFrame(data=d)\n",
    "final_df.to_csv(os.path.join(results_dir, 'test.csv'), index=False, header=True)\n",
    "# final_df\n",
    "\n",
    "# Write names of corrupted files for later use\n",
    "#with open(os.path.join(results_dir, 'missing.txt'), 'w') as f:\n",
    "#    for i in missing_images:\n",
    "#        f.write('{}\\n'.format(i))\n",
    "\n",
    "d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "d.update(top_label_probs)\n",
    "sorted_df = pd.DataFrame(data=d)\n",
    "sorted_df = sorted_df[['y_tile', 'x_tile', 'label1', 'prob1', 'label2', 'prob2', 'label3', 'prob3', 'label4', 'prob4', 'label5', 'prob5', 'label6', 'prob6', 'label7', 'prob7', 'label8', 'prob8', 'label9', 'prob9', 'label10', 'prob10']]\n",
    "sorted_df.to_csv(os.path.join(results_dir, 'top_predictions.csv'), index=False, header=True)\n",
    "# sorted_df.head()\n",
    "\n",
    "# preds = sorted(preds, key=lambda x: x[0])\n",
    "# for p_class, p_prob, truth in preds:\n",
    "#     got_it = label_names[p_class] == truth\n",
    "#     print('{}\\tPrediction: {}\\tTruth: {}\\tProb: {}'.format(got_it, label_names[p_class], truth, p_prob))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# test_images_dir = 'lhr/newtask'\n",
    "test_images_dir = 'Kabul2NepalDatasets/Kabul2NepalZoom20Y2018_5001_10000_Images/'\n",
    "\n",
    "# results_dir = 'results_lahore_17_with_old_wets_attempt2'\n",
    "results_dir = 'Results/Kabul2NepalZoom20Y2018_5001_10000_Images/'\n",
    "if not os.path.exists(results_dir):\n",
    "    print('Creating new results directory \"{}\"'.format(results_dir))\n",
    "    os.mkdir(results_dir)\n",
    "im_dir = os.path.join(results_dir, 'images')\n",
    "if not os.path.exists(im_dir):\n",
    "    os.mkdir(im_dir)\n",
    "\n",
    "#for file in os.listdir(test_images_dir):\n",
    "#    curr_name, curr_ext = os.path.splitext(file)\n",
    " #   print(curr_name[:-1])\n",
    " #   break\n",
    "#    new_name = curr_name[:-1]\n",
    "#    os.rename(os.path.join(test_images_dir, file), os.path.join(test_images_dir, new_name+curr_ext))\n",
    "\n",
    "########################################################\n",
    "########### RENAMING DONE IN DOWNLOAD SCRIPT ###########\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print('Waiting'len(images)len(images)len(images), end='', flush=True)len(images)\n",
    "# while True:\n",
    "    \n",
    " #   if len(os.listdir(test_images_dir)) != 544244:\n",
    " #      time.sleep(10*60)\n",
    " #       print('.', end='', flush=True)\n",
    " #       continue\n",
    "\n",
    "\n",
    "# if os.path.exists(os.path.join(results_dir, 'houses.txt')) or os.path.exists(os.path.join(results_dir, 'test.csv')):\n",
    "#     raise OSError('Result files already present, this script will append to the existing data.')\n",
    "    \n",
    "# label_names = {0: 'parking', 1: 'parks', 2: 'ground', 3: 'houses', 4: 'roads', 5: 'mosque', 6: 'densetrees', 7: 'kiln', 8: 'oiltanks', 9: 'tennis', 10: 'ponds', 11: 'grass', 12: 'blackfarms', 13: 'farms', 14: 'orchard'}\n",
    "# label_names = {0: 'parking', 1: 'parks', 2: 'ground', 3: 'houses', 4: 'roads', 5: 'mosque', 6: 'densetrees', 7: 'kiln', 8: 'oiltanks', 9: 'tennis', 10: 'ponds', 11: 'grass', 12: 'blackfarms', 13: 'farms'}\n",
    "# label_names = {0: 'blackfarms', 1: 'densetrees', 2: 'farms', 3: 'grass', 4: 'ground', 5: 'houses', 6: 'kiln', 7: 'mosque', 8: 'oiltanks', 9: 'orchard', 10: 'parking', 11: 'parks', 12: 'ponds', 13: 'roads', 14: 'tennis'}\n",
    "label_names = {0: 'blackfarms', 1: 'densetrees', 2: 'farms', 3: 'grass', 4: 'ground', 5: 'houses', 6: 'kiln', 7: 'orchard', 8: 'parking', 9: 'roads', 10:'unevenland'}\n",
    "y_tiles, x_tiles = [], []\n",
    "label_probs = defaultdict(list)\n",
    "top_label_probs = defaultdict(list)\n",
    "\n",
    "# # Resuming check\n",
    "# try:\n",
    "#     with open(os.path.join(results_dir, 'last_file_done.txt'), 'r') as f:\n",
    "#         loop_index = int(f.read())\n",
    "#     print('Resuming from file index \"{}\"'.format(loop_index))\n",
    "# except FileNotFoundError:\n",
    "#     loop_index = 0\n",
    "#     print('Starting fresh from file index \"0\"')\n",
    "\n",
    "filenames = os.listdir(test_images_dir)\n",
    "\n",
    "# # Missing images check\n",
    "# try:\n",
    "#     with open(os.path.join(results_dir, 'missing_images.txt'), 'r') as f:\n",
    "#         missing_images = [name.strip() for name in f.readlines()]\n",
    "#     print('Continuing with {} missing images.'.format(len(missing_images)))\n",
    "# except FileNotFoundError:\n",
    "#     missing_images = []\n",
    "#     print('Starting fresh with empty missing images list')\n",
    "for img_name in tqdm(filenames):\n",
    "\n",
    "\n",
    "#     extt=img_name.split('.')[1]\n",
    "    orig_name, extt = os.path.splitext(img_name)\n",
    "    if(extt == '.jpg'):\n",
    "        img_orig = cv2.imread(os.path.join(test_images_dir, img_name), cv2.IMREAD_COLOR)\n",
    "        # To catch corrupt images\n",
    "        if type(img_orig) == type(None):\n",
    "            print('Skipping an image \"{}\"'.format(img_name))\n",
    "        #    missing_images.append(img_name)\n",
    "            continue\n",
    "\n",
    "        img = np.expand_dims(img_orig, axis=0)\n",
    "        class_name = img_name.split('.')[0]\n",
    "\n",
    "        pred = model.predict(img, verbose=0)\n",
    "    #    preds.append((pred.argmax(), pred.max(), class_name))\n",
    "\n",
    "#         src = os.path.join(test_images_dir, img_name)\n",
    "#         dst = os.path.join(results_dir, 'images', orig_name+'_'+label_names[pred.argmax()]+extt)\n",
    "#         shutil.copyfile(src, dst)\n",
    "#         print(img_name, dst)\n",
    "\n",
    "#         break\n",
    "        x_tile, y_tile = os.path.splitext(img_name)[0].split('_')\n",
    "        y_tiles.append(y_tile)\n",
    "        x_tiles.append(x_tile)\n",
    "\n",
    "        # Append to individual results files on each itteration\n",
    "        for i, label in label_names.items():\n",
    "            file_path = os.path.join(results_dir, label+'.txt')\n",
    "            label_probs[label].append(pred[0, i])\n",
    "            with open(file_path, 'a') as f:\n",
    "                f.write('{} {} {}\\n'.format(y_tile, x_tile, pred[0, i]))\n",
    "#         # Append to full result file on every 1000 itterations\n",
    "#         #if loop_index % 1000 == 0:\n",
    "#         d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "#         d.update(label_probs)\n",
    "#         final_df = pd.DataFrame(data=d)\n",
    "#         final_df.to_csv(os.path.join(results_dir, 'test.csv'), index=False, header=True)\n",
    "        # Resuming \n",
    "        #with open('last_file_done.txt', 'w') as f:\n",
    "         #   f.write(str(loop_index))\n",
    "        # Write names of corrupted files for later use\n",
    "        #with open(os.path.join(results_dir, 'missing.txt'), 'w') as f:\n",
    "           # for i in missing_images:\n",
    "           #     f.write('{}\\n'.format(i))\n",
    "        #loop_index += 1\n",
    "\n",
    "        top3_inds = np.argsort(pred)\n",
    "        top3_inds = np.flip(top3_inds, axis=1)\n",
    "        top3_labels = [ label_names[i] for ia in top3_inds for i in ia ]\n",
    "        top3_probs = pred[0, top3_inds]\n",
    "#     print(top3_labels)\n",
    "\n",
    "        for n in range(num_classes):\n",
    "            top_label_probs['label'+str(n+1)].append(top3_labels[n])\n",
    "            top_label_probs['prob'+str(n+1)].append(top3_probs[0, n])\n",
    "\n",
    "# Append to full result file last time\n",
    "d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "d.update(label_probs)\n",
    "final_df = pd.DataFrame(data=d)\n",
    "final_df.to_csv(os.path.join(results_dir, 'test.csv'), index=False, header=True)\n",
    "# final_df\n",
    "\n",
    "# Write names of corrupted files for later use\n",
    "#with open(os.path.join(results_dir, 'missing.txt'), 'w') as f:\n",
    "#    for i in missing_images:\n",
    "#        f.write('{}\\n'.format(i))\n",
    "\n",
    "d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "d.update(top_label_probs)\n",
    "sorted_df = pd.DataFrame(data=d)\n",
    "sorted_df = sorted_df[['y_tile', 'x_tile', 'label1', 'prob1', 'label2', 'prob2', 'label3', 'prob3', 'label4', 'prob4', 'label5', 'prob5', 'label6', 'prob6', 'label7', 'prob7', 'label8', 'prob8', 'label9', 'prob9', 'label10', 'prob10']]\n",
    "sorted_df.to_csv(os.path.join(results_dir, 'top_predictions.csv'), index=False, header=True)\n",
    "# sorted_df.head()\n",
    "\n",
    "# preds = sorted(preds, key=lambda x: x[0])\n",
    "# for p_class, p_prob, truth in preds:\n",
    "#     got_it = label_names[p_class] == truth\n",
    "#     print('{}\\tPrediction: {}\\tTruth: {}\\tProb: {}'.format(got_it, label_names[p_class], truth, p_prob))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# test_images_dir = 'lhr/newtask'\n",
    "test_images_dir = 'Kabul2NepalDatasets/Kabul2NepalZoom20Y2018_10000_15000_Images/'\n",
    "\n",
    "# results_dir = 'results_lahore_17_with_old_wets_attempt2'\n",
    "results_dir = 'Kabul2NepalDatasets/Kabul2NepalZoom20Y2018_10000_15000_Images/'\n",
    "if not os.path.exists(results_dir):\n",
    "    print('Creating new results directory \"{}\"'.format(results_dir))\n",
    "    os.mkdir(results_dir)\n",
    "im_dir = os.path.join(results_dir, 'images')\n",
    "if not os.path.exists(im_dir):\n",
    "    os.mkdir(im_dir)\n",
    "\n",
    "#for file in os.listdir(test_images_dir):\n",
    "#    curr_name, curr_ext = os.path.splitext(file)\n",
    " #   print(curr_name[:-1])\n",
    " #   break\n",
    "#    new_name = curr_name[:-1]\n",
    "#    os.rename(os.path.join(test_images_dir, file), os.path.join(test_images_dir, new_name+curr_ext))\n",
    "\n",
    "########################################################\n",
    "########### RENAMING DONE IN DOWNLOAD SCRIPT ###########\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print('Waiting'len(images)len(images)len(images), end='', flush=True)len(images)\n",
    "# while True:\n",
    "    \n",
    " #   if len(os.listdir(test_images_dir)) != 544244:\n",
    " #      time.sleep(10*60)\n",
    " #       print('.', end='', flush=True)\n",
    " #       continue\n",
    "\n",
    "\n",
    "# if os.path.exists(os.path.join(results_dir, 'houses.txt')) or os.path.exists(os.path.join(results_dir, 'test.csv')):\n",
    "#     raise OSError('Result files already present, this script will append to the existing data.')\n",
    "    \n",
    "# label_names = {0: 'parking', 1: 'parks', 2: 'ground', 3: 'houses', 4: 'roads', 5: 'mosque', 6: 'densetrees', 7: 'kiln', 8: 'oiltanks', 9: 'tennis', 10: 'ponds', 11: 'grass', 12: 'blackfarms', 13: 'farms', 14: 'orchard'}\n",
    "# label_names = {0: 'parking', 1: 'parks', 2: 'ground', 3: 'houses', 4: 'roads', 5: 'mosque', 6: 'densetrees', 7: 'kiln', 8: 'oiltanks', 9: 'tennis', 10: 'ponds', 11: 'grass', 12: 'blackfarms', 13: 'farms'}\n",
    "# label_names = {0: 'blackfarms', 1: 'densetrees', 2: 'farms', 3: 'grass', 4: 'ground', 5: 'houses', 6: 'kiln', 7: 'mosque', 8: 'oiltanks', 9: 'orchard', 10: 'parking', 11: 'parks', 12: 'ponds', 13: 'roads', 14: 'tennis'}\n",
    "label_names = {0: 'blackfarms', 1: 'densetrees', 2: 'farms', 3: 'grass', 4: 'ground', 5: 'houses', 6: 'kiln', 7: 'orchard', 8: 'parking', 9: 'roads', 10:'unevenland'}\n",
    "y_tiles, x_tiles = [], []\n",
    "label_probs = defaultdict(list)\n",
    "top_label_probs = defaultdict(list)\n",
    "\n",
    "# # Resuming check\n",
    "# try:\n",
    "#     with open(os.path.join(results_dir, 'last_file_done.txt'), 'r') as f:\n",
    "#         loop_index = int(f.read())\n",
    "#     print('Resuming from file index \"{}\"'.format(loop_index))\n",
    "# except FileNotFoundError:\n",
    "#     loop_index = 0\n",
    "#     print('Starting fresh from file index \"0\"')\n",
    "\n",
    "filenames = os.listdir(test_images_dir)\n",
    "\n",
    "# # Missing images check\n",
    "# try:\n",
    "#     with open(os.path.join(results_dir, 'missing_images.txt'), 'r') as f:\n",
    "#         missing_images = [name.strip() for name in f.readlines()]\n",
    "#     print('Continuing with {} missing images.'.format(len(missing_images)))\n",
    "# except FileNotFoundError:\n",
    "#     missing_images = []\n",
    "#     print('Starting fresh with empty missing images list')\n",
    "for img_name in tqdm(filenames):\n",
    "\n",
    "\n",
    "#     extt=img_name.split('.')[1]\n",
    "    orig_name, extt = os.path.splitext(img_name)\n",
    "    if(extt == '.jpg'):\n",
    "        img_orig = cv2.imread(os.path.join(test_images_dir, img_name), cv2.IMREAD_COLOR)\n",
    "        # To catch corrupt images\n",
    "        if type(img_orig) == type(None):\n",
    "            print('Skipping an image \"{}\"'.format(img_name))\n",
    "        #    missing_images.append(img_name)\n",
    "            continue\n",
    "\n",
    "        img = np.expand_dims(img_orig, axis=0)\n",
    "        class_name = img_name.split('.')[0]\n",
    "\n",
    "        pred = model.predict(img, verbose=0)\n",
    "    #    preds.append((pred.argmax(), pred.max(), class_name))\n",
    "\n",
    "#         src = os.path.join(test_images_dir, img_name)\n",
    "#         dst = os.path.join(results_dir, 'images', orig_name+'_'+label_names[pred.argmax()]+extt)\n",
    "#         shutil.copyfile(src, dst)\n",
    "#         print(img_name, dst)\n",
    "\n",
    "#         break\n",
    "        x_tile, y_tile = os.path.splitext(img_name)[0].split('_')\n",
    "        y_tiles.append(y_tile)\n",
    "        x_tiles.append(x_tile)\n",
    "\n",
    "        # Append to individual results files on each itteration\n",
    "        for i, label in label_names.items():\n",
    "            file_path = os.path.join(results_dir, label+'.txt')\n",
    "            label_probs[label].append(pred[0, i])\n",
    "            with open(file_path, 'a') as f:\n",
    "                f.write('{} {} {}\\n'.format(y_tile, x_tile, pred[0, i]))\n",
    "#         # Append to full result file on every 1000 itterations\n",
    "#         #if loop_index % 1000 == 0:\n",
    "#         d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "#         d.update(label_probs)\n",
    "#         final_df = pd.DataFrame(data=d)\n",
    "#         final_df.to_csv(os.path.join(results_dir, 'test.csv'), index=False, header=True)\n",
    "        # Resuming \n",
    "        #with open('last_file_done.txt', 'w') as f:\n",
    "         #   f.write(str(loop_index))\n",
    "        # Write names of corrupted files for later use\n",
    "        #with open(os.path.join(results_dir, 'missing.txt'), 'w') as f:\n",
    "           # for i in missing_images:\n",
    "           #     f.write('{}\\n'.format(i))\n",
    "        #loop_index += 1\n",
    "\n",
    "        top3_inds = np.argsort(pred)\n",
    "        top3_inds = np.flip(top3_inds, axis=1)\n",
    "        top3_labels = [ label_names[i] for ia in top3_inds for i in ia ]\n",
    "        top3_probs = pred[0, top3_inds]\n",
    "#     print(top3_labels)\n",
    "\n",
    "        for n in range(num_classes):\n",
    "            top_label_probs['label'+str(n+1)].append(top3_labels[n])\n",
    "            top_label_probs['prob'+str(n+1)].append(top3_probs[0, n])\n",
    "\n",
    "# Append to full result file last time\n",
    "d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "d.update(label_probs)\n",
    "final_df = pd.DataFrame(data=d)\n",
    "final_df.to_csv(os.path.join(results_dir, 'test.csv'), index=False, header=True)\n",
    "# final_df\n",
    "\n",
    "# Write names of corrupted files for later use\n",
    "#with open(os.path.join(results_dir, 'missing.txt'), 'w') as f:\n",
    "#    for i in missing_images:\n",
    "#        f.write('{}\\n'.format(i))\n",
    "\n",
    "d = {'y_tile': y_tiles, 'x_tile': x_tiles}\n",
    "d.update(top_label_probs)\n",
    "sorted_df = pd.DataFrame(data=d)\n",
    "sorted_df = sorted_df[['y_tile', 'x_tile', 'label1', 'prob1', 'label2', 'prob2', 'label3', 'prob3', 'label4', 'prob4', 'label5', 'prob5', 'label6', 'prob6', 'label7', 'prob7', 'label8', 'prob8', 'label9', 'prob9', 'label10', 'prob10']]\n",
    "sorted_df.to_csv(os.path.join(results_dir, 'top_predictions.csv'), index=False, header=True)\n",
    "# sorted_df.head()\n",
    "\n",
    "# preds = sorted(preds, key=lambda x: x[0])\n",
    "# for p_class, p_prob, truth in preds:\n",
    "#     got_it = label_names[p_class] == truth\n",
    "#     print('{}\\tPrediction: {}\\tTruth: {}\\tProb: {}'.format(got_it, label_names[p_class], truth, p_prob))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
